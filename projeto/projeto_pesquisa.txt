Projeto de Pesquisa – Doutorado Vitor

Sumário
TÍTULO	1
INTRODUÇÃO E CONTEXTUALIZAÇÃO	1
PROBLEMA DE PESQUISA E OBJETIVOS	2
Problema Central	2
Pergunta Principal de Pesquisa (P1)	2
Perguntas Secundárias (P2-P4)	2
Objetivo Geral	2
Objetivos Específicos	2
JUSTIFICATIVA E RELEVÂNCIA	3
FUNDAMENTAÇÃO TEÓRICA (REVISÃO DA LITERATURA)	3
METODOLOGIA PROPOSTA	3
Fase 1: Definição do Domínio e Taxonomia de Tarefas (OE1)	4
Fase 2: Design do Benchmark Experimental (OE2)	4
Fase 3: Execução Experimental (OE3)	4
Fase 4: Análise e Mapeamento da Fronteira (OE4, OE5)	4
CRONOGRAMA PRELIMINAR (48 MESES)	5
REFERÊNCIAS BIBLIOGRÁFICAS PRELIMINARES	5


Título
Mapeando a Fronteira Irregular: Uma Análise Experimental da Capacidade de Agentes Baseados em LLM em Tarefas Complexas de Engenharia de Poços Offshore
(Mapping the Jagged Frontier: An Experimental Analysis of LLM-Based Agent Capabilities in Well Engineering Tasks)

Introdução e Contextualização
A ascensão dos Grandes Modelos de Linguagem (LLMs) marca uma transição da IA como ferramenta de assistência para a IA como agente autônomo. Sistemas de agentes (ex: baseados em RAG ou ReAct) agora podem decompor problemas, planejar e executar tarefas de múltiplos passos.
Contudo, a rápida adoção dessa tecnologia na indústria, especialmente em setores de alto risco como Óleo e Gás (O&G), supera nossa compreensão de seus reais limites. O entusiasmo com as capacidades de "pico" (sucesso impressionante) muitas vezes ofusca a existência de "vales" (falhas inesperadas e ilógicas) em suas capacidades.
O influente estudo de Mollick et al. (2023) com o BCG introduziu o conceito de "Fronteira Tecnológica Irregular" (Jagged Frontier) para descrever como o desempenho da IA é irregular, alternando entre picos de competência super-humana e vales de falha. No entanto, este estudo focou na produtividade de humanos usando IA em tarefas de consultoria.
A lacuna que este projeto aborda é: a fronteira irregular para agentes autônomos em domínios de engenharia de alta complexidade permanece desconhecida. Não sabemos quais tarefas na construção de poços offshore (um domínio que não permite falhas) estão nos "picos" e quais estão nos "vales".

Problema de Pesquisa e Objetivos

Problema Central
A implantação de agentes de LLM em operações críticas de O&G é dificultada pela falta de um mapa de risco-capacidade. As métricas de benchmarks genéricos (ex: MMLU, AgentBench) não capturam as nuances de tarefas de engenharia do mundo real, que envolvem dados ruidosos, raciocínio físico e adesão estrita a normas de segurança.

Pergunta Principal de Pesquisa (P1)
Onde se localiza, e qual é a topografia, da "fronteira irregular" de capacidade para agentes de LLM no domínio de planejamento e execução de tarefas da construção de poços offshore?

Perguntas Secundárias (P2-P4)
•	(P2) Quais características de uma tarefa (ex: necessidade de raciocínio causal, dependência de dados físicos, conformidade regulatória, planejamento temporal) definem um "pico" (sucesso do agente) ou um "vale" (falha do agente)?
•	(P3) Como diferentes arquiteturas de agentes (ex: LLM "puro" vs. RAG vs. Agentes de Planejamento) navegam por essa fronteira?
•	(P4) É possível desenvolver um framework para identificar "vales" a priori, permitindo a implantação segura de agentes em tarefas de "pico"?
Objetivo Geral
Mapear e caracterizar a fronteira irregular de capacidade de agentes de LLM no domínio de engenharia de poços, identificando os fatores que determinam o sucesso e a falha em tarefas complexas.
Objetivos Específicos
1.	OE1: Desenvolver uma taxonomia de tarefas representativas da construção de poços offshore, classificadas por tipo de cognição e complexidade.
2.	OE2: Projetar e implementar um benchmark experimental baseado nesta taxonomia, com métricas de avaliação e ground truth definidos por especialistas.
3.	OE3: Avaliar sistematicamente diferentes arquiteturas de agentes de LLM (ex: GPT-4o, Claude 3, Llama 3 com diferentes frameworks de agência) neste benchmark.
4.	OE4: Analisar os resultados para construir o "mapa" da fronteira irregular, correlacionando tipos de tarefa com o desempenho dos agentes.
5.	OE5: Propor um framework de decisão para a implantação segura de agentes na indústria de O&G, baseado nas descobertas.

Justificativa e Relevância
Este projeto possui relevância em três eixos:
1.	Contribuição para a Ciência da Computação (Teórica): Estende a teoria da "Fronteira Irregular" do campo de Interação Humano-Computador (HCI) para o campo de Agentes Autônomos. Além disso, critica e avança o estado da arte em benchmarking de agentes, saindo de tarefas genéricas para domínios industriais complexos.
2.	Contribuição para a Indústria de O&G (Prática): Fornece o primeiro estudo rigoroso sobre o que agentes de IA podem (e, crucialmente, não podem) fazer com segurança na engenharia de poços. Isso desbloqueia ganhos de eficiência (em "picos") e previne falhas catastróficas (em "vales").
3.	Originalidade: A intersecção de Agentes de IA, a teoria da "Jagged Frontier" e o domínio de O&G é inteiramente nova na literatura.

Fundamentação Teórica (Revisão da Literatura)
A tese será fundamentada em quatro pilares:
1.	Agentes Baseados em LLM: Arquiteturas e paradigmas (RAG, ReAct, CoT, Multi-Agentes). Como eles funcionam, planejam e usam ferramentas.
2.	Avaliação de Agentes (Benchmarking): Estado da arte (ex: AgentBench, GAIA, MT-Bench). Análise de suas limitações para tarefas industriais/engenharia.
3.	Produtividade e Limites da IA: O paper seminal de Brynjolfsson, Mollick, et al. (2023) sobre a "Fronteira Irregular".
4.	Engenharia de Poços e IA: Aplicações atuais de machine learning em O&G (ex: manutenção preditiva, análise sísmica) e a lacuna existente na aplicação de agentes generativos para planejamento operacional.

Metodologia Proposta
Este projeto empregará uma metodologia de pesquisa experimental quantitativa e qualitativa, dividida em quatro fases:

Fase 1: Definição do Domínio e Taxonomia de Tarefas (OE1)
•	Fonte de Dados: Análise documental de Manuais de Engenharia de Perfuração, Procedimentos Operacionais Padrão (POPs), e Relatórios Diários de Perfuração (DDRs/Boletins Diários de Operação - BDOs).
•	Amostragem: Criação de um dataset de 50-100 tarefas representativas.
•	Classificação (Taxonomia): As tarefas serão classificadas por eixos:
o	Tipo de Ação: Extração de Informação, Síntese, Diagnóstico, Planejamento, Verificação de Conformidade.
o	Domínio de Conhecimento: Geologia, Fluidos, Mecânica, Regulação.
o	Complexidade: Nível de raciocínio causal, temporal e espacial exigido.

Fase 2: Design do Benchmark Experimental (OE2)
•	Plataforma: Desenvolvimento de um ambiente de teste (sandbox) onde os agentes podem atuar.
•	Ferramentas (Tools): Disponibilização de "ferramentas" simuladas para os agentes (ex: buscar_norma_api(id), calcular_volume_anular(diametros), ler_ultimo_ddr()).
•	Ground Truth: Definição de critérios de sucesso (o "gabarito") para cada tarefa, validado por Especialistas no Domínio (SMEs - Subject Matter Experts).

Fase 3: Execução Experimental (OE3)
•	Variáveis Independentes: Arquitetura do Agente (Ex: Baseline GPT-4o, RAG c/ base de POPs, Agente ReAct).
•	Variáveis Dependentes (Métricas):
1.	Taxa de Sucesso Binário: Completou a tarefa com sucesso?
2.	Qualidade da Resposta: Avaliação cega (1-5) por SMEs.
3.	Eficiência: Custo (tokens), passos de raciocínio.
4.	Robustez: O agente "alucina" ou falha graciosamente?

Fase 4: Análise e Mapeamento da Fronteira (OE4, OE5)
•	Análise Quantitativa: Correlação estatística entre as características da tarefa (da Fase 1) e as métricas de desempenho (da Fase 3).
•	Análise Qualitativa: Análise de causa-raiz das falhas ("vales"). O agente falhou por falta de conhecimento (RAG falho), raciocínio (LLM falho) ou planejamento (arquitetura falha)?
•	Resultado: O "mapa" da fronteira e o framework de decisão.

Cronograma Preliminar (48 meses)
•	Ano 1 (Meses 1-12):
o	Revisão de Literatura aprofundada.
o	Disciplinas obrigatórias.
o	Execução da Fase 1 (Taxonomia de Tarefas).
o	Definição do projeto de tese (Exame de Qualificação).
•	Ano 2 (Meses 13-24):
o	Execução da Fase 2 (Desenvolvimento do Benchmark).
o	Testes-piloto.
o	Artigo de revisão ou position paper sobre o benchmark.
•	Ano 3 (Meses 25-36):
o	Execução da Fase 3 (Bateria principal de testes e avaliação).
o	Coleta de dados (avaliação pelos SMEs).
o	Início da Fase 4 (Análise).
o	Submissão de artigo para conferência principal (ex: NeurIPS, ICML, ou conferência de O&G como a OTC).
•	Ano 4 (Meses 37-48):
o	Conclusão da Fase 4 (Desenvolvimento do Framework).
o	Redação da Tese.
o	Submissão de artigo em periódico.
o	Defesa.

Referências Bibliográficas Preliminares
•	Mollick, E., & Brynjolfsson, E., et al. (2023). Navigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI on Knowledge Worker Productivity and Quality. HBS Working Paper.
•	Yao, S., et al. (2023). ReAct: Synergizing Reasoning and Acting in Language Models.
•	Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.
•	Zeng, Y., et al. (2024). AgentBench: Evaluating LLMs as Agents.
•	... (Outros sobre IA em O&G e metodologias de avaliação de LLMs)

